{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 16\n",
    "output_dim = 2  \n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "graph_dir = \"processed_graphs\"\n",
    "embeddings_dir = \"graph_embeddings\"\n",
    "os.makedirs(embeddings_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN for year 2000...\n",
      "Year 2000, Epoch 0: Loss = 515.8576\n",
      "Year 2000, Epoch 10: Loss = 124.3597\n",
      "Year 2000, Epoch 20: Loss = 7.7808\n",
      "Year 2000, Epoch 30: Loss = 13.0984\n",
      "Year 2000, Epoch 40: Loss = 4.9510\n",
      "Year 2000, Epoch 50: Loss = 4.5445\n",
      "Year 2000, Epoch 60: Loss = 3.2599\n",
      "Year 2000, Epoch 70: Loss = 3.3720\n",
      "Year 2000, Epoch 80: Loss = 3.1253\n",
      "Year 2000, Epoch 90: Loss = 3.0841\n",
      "Embeddings saved for 2000!\n",
      "Training GCN for year 2001...\n",
      "Year 2001, Epoch 0: Loss = 219.1116\n",
      "Year 2001, Epoch 10: Loss = 19.7605\n",
      "Year 2001, Epoch 20: Loss = 16.3677\n",
      "Year 2001, Epoch 30: Loss = 5.9628\n",
      "Year 2001, Epoch 40: Loss = 5.5465\n",
      "Year 2001, Epoch 50: Loss = 4.0447\n",
      "Year 2001, Epoch 60: Loss = 3.8068\n",
      "Year 2001, Epoch 70: Loss = 3.5153\n",
      "Year 2001, Epoch 80: Loss = 3.3617\n",
      "Year 2001, Epoch 90: Loss = 3.3574\n",
      "Embeddings saved for 2001!\n",
      "Training GCN for year 2002...\n",
      "Year 2002, Epoch 0: Loss = 615.5639\n",
      "Year 2002, Epoch 10: Loss = 424.7521\n",
      "Year 2002, Epoch 20: Loss = 273.7277\n",
      "Year 2002, Epoch 30: Loss = 78.7131\n",
      "Year 2002, Epoch 40: Loss = 17.8675\n",
      "Year 2002, Epoch 50: Loss = 16.8501\n",
      "Year 2002, Epoch 60: Loss = 7.8193\n",
      "Year 2002, Epoch 70: Loss = 4.4351\n",
      "Year 2002, Epoch 80: Loss = 3.8284\n",
      "Year 2002, Epoch 90: Loss = 3.5780\n",
      "Embeddings saved for 2002!\n",
      "Training GCN for year 2003...\n",
      "Year 2003, Epoch 0: Loss = 427.0999\n",
      "Year 2003, Epoch 10: Loss = 107.9925\n",
      "Year 2003, Epoch 20: Loss = 6.0331\n",
      "Year 2003, Epoch 30: Loss = 18.8245\n",
      "Year 2003, Epoch 40: Loss = 5.1473\n",
      "Year 2003, Epoch 50: Loss = 5.1246\n",
      "Year 2003, Epoch 60: Loss = 3.7184\n",
      "Year 2003, Epoch 70: Loss = 3.4773\n",
      "Year 2003, Epoch 80: Loss = 3.3288\n",
      "Year 2003, Epoch 90: Loss = 3.2547\n",
      "Embeddings saved for 2003!\n",
      "Training GCN for year 2004...\n",
      "Year 2004, Epoch 0: Loss = 521.3705\n",
      "Year 2004, Epoch 10: Loss = 122.1856\n",
      "Year 2004, Epoch 20: Loss = 12.7684\n",
      "Year 2004, Epoch 30: Loss = 15.9368\n",
      "Year 2004, Epoch 40: Loss = 8.7409\n",
      "Year 2004, Epoch 50: Loss = 4.6424\n",
      "Year 2004, Epoch 60: Loss = 3.5432\n",
      "Year 2004, Epoch 70: Loss = 3.0192\n",
      "Year 2004, Epoch 80: Loss = 2.9724\n",
      "Year 2004, Epoch 90: Loss = 2.8923\n",
      "Embeddings saved for 2004!\n",
      "Training GCN for year 2005...\n",
      "Year 2005, Epoch 0: Loss = 380.5963\n",
      "Year 2005, Epoch 10: Loss = 17.5175\n",
      "Year 2005, Epoch 20: Loss = 25.9554\n",
      "Year 2005, Epoch 30: Loss = 5.3539\n",
      "Year 2005, Epoch 40: Loss = 7.2344\n",
      "Year 2005, Epoch 50: Loss = 3.7316\n",
      "Year 2005, Epoch 60: Loss = 3.5533\n",
      "Year 2005, Epoch 70: Loss = 3.1830\n",
      "Year 2005, Epoch 80: Loss = 2.9870\n",
      "Year 2005, Epoch 90: Loss = 2.9682\n",
      "Embeddings saved for 2005!\n",
      "Training GCN for year 2006...\n",
      "Year 2006, Epoch 0: Loss = 715.4387\n",
      "Year 2006, Epoch 10: Loss = 175.3659\n",
      "Year 2006, Epoch 20: Loss = 22.3147\n",
      "Year 2006, Epoch 30: Loss = 20.7606\n",
      "Year 2006, Epoch 40: Loss = 13.8702\n",
      "Year 2006, Epoch 50: Loss = 8.5784\n",
      "Year 2006, Epoch 60: Loss = 8.8406\n",
      "Year 2006, Epoch 70: Loss = 7.6136\n",
      "Year 2006, Epoch 80: Loss = 7.4187\n",
      "Year 2006, Epoch 90: Loss = 7.0936\n",
      "Embeddings saved for 2006!\n",
      "Training GCN for year 2007...\n",
      "Year 2007, Epoch 0: Loss = 1050.2672\n",
      "Year 2007, Epoch 10: Loss = 392.1737\n",
      "Year 2007, Epoch 20: Loss = 82.9308\n",
      "Year 2007, Epoch 30: Loss = 15.6589\n",
      "Year 2007, Epoch 40: Loss = 14.8024\n",
      "Year 2007, Epoch 50: Loss = 6.4781\n",
      "Year 2007, Epoch 60: Loss = 5.0274\n",
      "Year 2007, Epoch 70: Loss = 4.5705\n",
      "Year 2007, Epoch 80: Loss = 3.9511\n",
      "Year 2007, Epoch 90: Loss = 3.8342\n",
      "Embeddings saved for 2007!\n",
      "Training GCN for year 2008...\n",
      "Year 2008, Epoch 0: Loss = 392.5943\n",
      "Year 2008, Epoch 10: Loss = 83.6805\n",
      "Year 2008, Epoch 20: Loss = 3.7145\n",
      "Year 2008, Epoch 30: Loss = 14.9568\n",
      "Year 2008, Epoch 40: Loss = 3.4017\n",
      "Year 2008, Epoch 50: Loss = 4.1904\n",
      "Year 2008, Epoch 60: Loss = 2.9545\n",
      "Year 2008, Epoch 70: Loss = 2.9118\n",
      "Year 2008, Epoch 80: Loss = 2.7702\n",
      "Year 2008, Epoch 90: Loss = 2.7662\n",
      "Embeddings saved for 2008!\n",
      "Training GCN for year 2009...\n",
      "Year 2009, Epoch 0: Loss = 1149.6482\n",
      "Year 2009, Epoch 10: Loss = 395.7583\n",
      "Year 2009, Epoch 20: Loss = 60.8451\n",
      "Year 2009, Epoch 30: Loss = 7.0421\n",
      "Year 2009, Epoch 40: Loss = 17.1749\n",
      "Year 2009, Epoch 50: Loss = 3.1016\n",
      "Year 2009, Epoch 60: Loss = 4.4278\n",
      "Year 2009, Epoch 70: Loss = 3.1589\n",
      "Year 2009, Epoch 80: Loss = 3.1181\n",
      "Year 2009, Epoch 90: Loss = 2.9496\n",
      "Embeddings saved for 2009!\n",
      "Training GCN for year 2010...\n",
      "Year 2010, Epoch 0: Loss = 677.9393\n",
      "Year 2010, Epoch 10: Loss = 316.3067\n",
      "Year 2010, Epoch 20: Loss = 93.1532\n",
      "Year 2010, Epoch 30: Loss = 7.2804\n",
      "Year 2010, Epoch 40: Loss = 18.8558\n",
      "Year 2010, Epoch 50: Loss = 5.9043\n",
      "Year 2010, Epoch 60: Loss = 6.4319\n",
      "Year 2010, Epoch 70: Loss = 5.1725\n",
      "Year 2010, Epoch 80: Loss = 4.7515\n",
      "Year 2010, Epoch 90: Loss = 4.4612\n",
      "Embeddings saved for 2010!\n",
      "Training GCN for year 2011...\n",
      "Year 2011, Epoch 0: Loss = 472.3230\n",
      "Year 2011, Epoch 10: Loss = 186.5439\n",
      "Year 2011, Epoch 20: Loss = 44.5328\n",
      "Year 2011, Epoch 30: Loss = 6.3282\n",
      "Year 2011, Epoch 40: Loss = 11.4688\n",
      "Year 2011, Epoch 50: Loss = 4.6717\n",
      "Year 2011, Epoch 60: Loss = 5.1663\n",
      "Year 2011, Epoch 70: Loss = 4.3762\n",
      "Year 2011, Epoch 80: Loss = 4.1550\n",
      "Year 2011, Epoch 90: Loss = 3.9525\n",
      "Embeddings saved for 2011!\n",
      "Training GCN for year 2012...\n",
      "Year 2012, Epoch 0: Loss = 317.2661\n",
      "Year 2012, Epoch 10: Loss = 100.6362\n",
      "Year 2012, Epoch 20: Loss = 13.2585\n",
      "Year 2012, Epoch 30: Loss = 6.6694\n",
      "Year 2012, Epoch 40: Loss = 8.4881\n",
      "Year 2012, Epoch 50: Loss = 4.5969\n",
      "Year 2012, Epoch 60: Loss = 4.4302\n",
      "Year 2012, Epoch 70: Loss = 4.2774\n",
      "Year 2012, Epoch 80: Loss = 3.9964\n",
      "Year 2012, Epoch 90: Loss = 3.9082\n",
      "Embeddings saved for 2012!\n",
      "Training GCN for year 2013...\n",
      "Year 2013, Epoch 0: Loss = 956.3344\n",
      "Year 2013, Epoch 10: Loss = 349.4515\n",
      "Year 2013, Epoch 20: Loss = 82.4910\n",
      "Year 2013, Epoch 30: Loss = 4.8925\n",
      "Year 2013, Epoch 40: Loss = 14.6477\n",
      "Year 2013, Epoch 50: Loss = 6.8529\n",
      "Year 2013, Epoch 60: Loss = 4.0366\n",
      "Year 2013, Epoch 70: Loss = 4.3391\n",
      "Year 2013, Epoch 80: Loss = 3.5910\n",
      "Year 2013, Epoch 90: Loss = 3.4806\n",
      "Embeddings saved for 2013!\n",
      "Training GCN for year 2014...\n",
      "Year 2014, Epoch 0: Loss = 431.3796\n",
      "Year 2014, Epoch 10: Loss = 222.6609\n",
      "Year 2014, Epoch 20: Loss = 100.7894\n",
      "Year 2014, Epoch 30: Loss = 27.5128\n",
      "Year 2014, Epoch 40: Loss = 6.3715\n",
      "Year 2014, Epoch 50: Loss = 7.8986\n",
      "Year 2014, Epoch 60: Loss = 7.1069\n",
      "Year 2014, Epoch 70: Loss = 5.5879\n",
      "Year 2014, Epoch 80: Loss = 5.5065\n",
      "Year 2014, Epoch 90: Loss = 5.3626\n",
      "Embeddings saved for 2014!\n",
      "Training GCN for year 2015...\n",
      "Year 2015, Epoch 0: Loss = 239.6833\n",
      "Year 2015, Epoch 10: Loss = 43.9313\n",
      "Year 2015, Epoch 20: Loss = 6.6773\n",
      "Year 2015, Epoch 30: Loss = 10.8862\n",
      "Year 2015, Epoch 40: Loss = 3.3789\n",
      "Year 2015, Epoch 50: Loss = 3.5624\n",
      "Year 2015, Epoch 60: Loss = 2.8238\n",
      "Year 2015, Epoch 70: Loss = 2.8578\n",
      "Year 2015, Epoch 80: Loss = 2.7915\n",
      "Year 2015, Epoch 90: Loss = 2.7640\n",
      "Embeddings saved for 2015!\n",
      "Training GCN for year 2016...\n",
      "Year 2016, Epoch 0: Loss = 268.2397\n",
      "Year 2016, Epoch 10: Loss = 13.7212\n",
      "Year 2016, Epoch 20: Loss = 25.4783\n",
      "Year 2016, Epoch 30: Loss = 8.4079\n",
      "Year 2016, Epoch 40: Loss = 9.4373\n",
      "Year 2016, Epoch 50: Loss = 6.5909\n",
      "Year 2016, Epoch 60: Loss = 6.1252\n",
      "Year 2016, Epoch 70: Loss = 5.3050\n",
      "Year 2016, Epoch 80: Loss = 4.8060\n",
      "Year 2016, Epoch 90: Loss = 4.4046\n",
      "Embeddings saved for 2016!\n",
      "Training GCN for year 2017...\n",
      "Year 2017, Epoch 0: Loss = 1723.3990\n",
      "Year 2017, Epoch 10: Loss = 735.7947\n",
      "Year 2017, Epoch 20: Loss = 229.6553\n",
      "Year 2017, Epoch 30: Loss = 31.2501\n",
      "Year 2017, Epoch 40: Loss = 18.8145\n",
      "Year 2017, Epoch 50: Loss = 13.4128\n",
      "Year 2017, Epoch 60: Loss = 7.0391\n",
      "Year 2017, Epoch 70: Loss = 6.5734\n",
      "Year 2017, Epoch 80: Loss = 5.5263\n",
      "Year 2017, Epoch 90: Loss = 5.0408\n",
      "Embeddings saved for 2017!\n",
      "Training GCN for year 2018...\n",
      "Year 2018, Epoch 0: Loss = 808.1099\n",
      "Year 2018, Epoch 10: Loss = 363.2666\n",
      "Year 2018, Epoch 20: Loss = 84.9623\n",
      "Year 2018, Epoch 30: Loss = 6.3432\n",
      "Year 2018, Epoch 40: Loss = 18.6691\n",
      "Year 2018, Epoch 50: Loss = 4.9309\n",
      "Year 2018, Epoch 60: Loss = 5.6072\n",
      "Year 2018, Epoch 70: Loss = 4.3086\n",
      "Year 2018, Epoch 80: Loss = 3.9243\n",
      "Year 2018, Epoch 90: Loss = 3.7032\n",
      "Embeddings saved for 2018!\n",
      "Training GCN for year 2019...\n",
      "Year 2019, Epoch 0: Loss = 174.8833\n",
      "Year 2019, Epoch 10: Loss = 19.6153\n",
      "Year 2019, Epoch 20: Loss = 15.5339\n",
      "Year 2019, Epoch 30: Loss = 5.6337\n",
      "Year 2019, Epoch 40: Loss = 6.0352\n",
      "Year 2019, Epoch 50: Loss = 4.0195\n",
      "Year 2019, Epoch 60: Loss = 3.7765\n",
      "Year 2019, Epoch 70: Loss = 3.2374\n",
      "Year 2019, Epoch 80: Loss = 3.0443\n",
      "Year 2019, Epoch 90: Loss = 2.9311\n",
      "Embeddings saved for 2019!\n",
      "Training GCN for year 2020...\n",
      "Year 2020, Epoch 0: Loss = 639.4964\n",
      "Year 2020, Epoch 10: Loss = 105.0563\n",
      "Year 2020, Epoch 20: Loss = 11.9722\n",
      "Year 2020, Epoch 30: Loss = 24.0049\n",
      "Year 2020, Epoch 40: Loss = 5.1826\n",
      "Year 2020, Epoch 50: Loss = 7.1186\n",
      "Year 2020, Epoch 60: Loss = 4.8377\n",
      "Year 2020, Epoch 70: Loss = 4.7110\n",
      "Year 2020, Epoch 80: Loss = 4.2549\n",
      "Year 2020, Epoch 90: Loss = 4.0852\n",
      "Embeddings saved for 2020!\n",
      "Training GCN for year 2021...\n",
      "Year 2021, Epoch 0: Loss = 425.3235\n",
      "Year 2021, Epoch 10: Loss = 234.6898\n",
      "Year 2021, Epoch 20: Loss = 93.5430\n",
      "Year 2021, Epoch 30: Loss = 16.7724\n",
      "Year 2021, Epoch 40: Loss = 9.5114\n",
      "Year 2021, Epoch 50: Loss = 10.6221\n",
      "Year 2021, Epoch 60: Loss = 6.7324\n",
      "Year 2021, Epoch 70: Loss = 6.7497\n",
      "Year 2021, Epoch 80: Loss = 6.4219\n",
      "Year 2021, Epoch 90: Loss = 6.1442\n",
      "Embeddings saved for 2021!\n",
      "Training GCN for year 2022...\n",
      "Year 2022, Epoch 0: Loss = 651.1212\n",
      "Year 2022, Epoch 10: Loss = 298.7644\n",
      "Year 2022, Epoch 20: Loss = 93.0298\n",
      "Year 2022, Epoch 30: Loss = 10.6116\n",
      "Year 2022, Epoch 40: Loss = 12.2299\n",
      "Year 2022, Epoch 50: Loss = 6.7561\n",
      "Year 2022, Epoch 60: Loss = 4.7667\n",
      "Year 2022, Epoch 70: Loss = 4.4936\n",
      "Year 2022, Epoch 80: Loss = 4.0832\n",
      "Year 2022, Epoch 90: Loss = 3.9311\n",
      "Embeddings saved for 2022!\n",
      "Training GCN for year 2023...\n",
      "Year 2023, Epoch 0: Loss = 235.5562\n",
      "Year 2023, Epoch 10: Loss = 15.6248\n",
      "Year 2023, Epoch 20: Loss = 18.2635\n",
      "Year 2023, Epoch 30: Loss = 4.3308\n",
      "Year 2023, Epoch 40: Loss = 5.7095\n",
      "Year 2023, Epoch 50: Loss = 3.9887\n",
      "Year 2023, Epoch 60: Loss = 3.9070\n",
      "Year 2023, Epoch 70: Loss = 3.5329\n",
      "Year 2023, Epoch 80: Loss = 3.2878\n",
      "Year 2023, Epoch 90: Loss = 3.0958\n",
      "Embeddings saved for 2023!\n",
      "GCN Training Completed!\n"
     ]
    }
   ],
   "source": [
    "for year in range(2000, 2024):\n",
    "    print(f\"Training GCN for year {year}...\")\n",
    "    graph = torch.load(f\"{graph_dir}/graph_{year}.pt\")\n",
    "    \n",
    "    model = GCN(in_channels=graph.x.shape[1], hidden_channels=hidden_dim, out_channels=output_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(graph.x, graph.edge_index)\n",
    "        loss = F.mse_loss(output, graph.x)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Year {year}, Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    \n",
    "    torch.save(output, f\"{embeddings_dir}/embeddings_{year}.pt\")\n",
    "    print(f\"Embeddings saved for {year}!\")\n",
    "\n",
    "print(\"GCN Training Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 483.0508\n",
      "Epoch 10: Loss = 449.6117\n",
      "Epoch 20: Loss = 388.9518\n",
      "Epoch 30: Loss = 334.7071\n",
      "Epoch 40: Loss = 287.6370\n",
      "Epoch 50: Loss = 246.4783\n",
      "Epoch 60: Loss = 210.6407\n",
      "Epoch 70: Loss = 179.5119\n",
      "Epoch 80: Loss = 152.5101\n",
      "Epoch 90: Loss = 129.1257\n",
      "Epoch 100: Loss = 108.9208\n",
      "Epoch 110: Loss = 91.5160\n",
      "Epoch 120: Loss = 76.5779\n",
      "Epoch 130: Loss = 63.8101\n",
      "Epoch 140: Loss = 52.9466\n",
      "Epoch 150: Loss = 43.7482\n",
      "Epoch 160: Loss = 35.9992\n",
      "Epoch 170: Loss = 29.5056\n",
      "Epoch 180: Loss = 24.0936\n",
      "Epoch 190: Loss = 19.6081\n",
      "Epoch 200: Loss = 15.9115\n",
      "Epoch 210: Loss = 12.8824\n",
      "Epoch 220: Loss = 10.4148\n",
      "Epoch 230: Loss = 8.4162\n",
      "Epoch 240: Loss = 6.8069\n",
      "Epoch 250: Loss = 5.5187\n",
      "Epoch 260: Loss = 4.4934\n",
      "Epoch 270: Loss = 3.6822\n",
      "Epoch 280: Loss = 3.0440\n",
      "Epoch 290: Loss = 2.5448\n",
      "LSTM Training Completed & Predictions Saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# Define LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        output = self.fc(lstm_out[:, -1, :])  # Use only the final timestep\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_dim = 16\n",
    "output_dim = 2\n",
    "num_layers = 2\n",
    "num_epochs = 300\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Load embeddings\n",
    "embeddings_dir = \"graph_embeddings\"\n",
    "years = list(range(2000, 2024))\n",
    "num_past_years = 5\n",
    "\n",
    "# Prepare dataset\n",
    "sequences = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(years) - num_past_years):\n",
    "    past_years = years[i : i + num_past_years]\n",
    "    future_year = years[i + num_past_years]\n",
    "    \n",
    "    past_embeddings = [torch.load(f\"{embeddings_dir}/embeddings_{y}.pt\") for y in past_years]\n",
    "    future_embedding = torch.load(f\"{embeddings_dir}/embeddings_{future_year}.pt\")\n",
    "    \n",
    "    past_embeddings = torch.stack(past_embeddings)  # Shape: (10, num_nodes, 8)\n",
    "    future_embedding = future_embedding  # Shape: (num_nodes, 8)\n",
    "    \n",
    "    sequences.append(past_embeddings)\n",
    "    targets.append(future_embedding)\n",
    "\n",
    "# Convert to tensors\n",
    "sequences = torch.stack(sequences)  # Shape: (num_samples, 10, num_nodes, 8)\n",
    "targets = torch.stack(targets)      # Shape: (num_samples, num_nodes, 8)\n",
    "\n",
    "# Reshape to match LSTM expectations\n",
    "num_samples, seq_len, num_nodes, emb_dim = sequences.shape\n",
    "sequences = sequences.view(num_samples, seq_len, num_nodes * emb_dim)\n",
    "targets = targets.view(num_samples, num_nodes * emb_dim)\n",
    "\n",
    "# Corrected Input Dimension\n",
    "input_dim = sequences.shape[-1]  # Dynamically set input size\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=targets.shape[-1], num_layers=num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(sequences)\n",
    "    loss = criterion(output, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# Save Prediction for 2024\n",
    "predicted_embedding_2024 = output.view(num_samples, num_nodes, emb_dim)  # Reshape back\n",
    "torch.save(predicted_embedding_2024, f\"{embeddings_dir}/predicted_embeddings_2024.pt\")\n",
    "\n",
    "print(\"LSTM Training Completed & Predictions Saved!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
