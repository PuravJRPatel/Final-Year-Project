{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 16\n",
    "output_dim = 2  \n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "graph_dir = \"processed_graphs\"\n",
    "embeddings_dir = \"graph_embeddings\"\n",
    "os.makedirs(embeddings_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN for year 2000...\n",
      "Year 2000, Epoch 0: Loss = 836.5386\n",
      "Year 2000, Epoch 10: Loss = 324.8958\n",
      "Year 2000, Epoch 20: Loss = 58.8775\n",
      "Year 2000, Epoch 30: Loss = 9.1099\n",
      "Year 2000, Epoch 40: Loss = 16.4582\n",
      "Year 2000, Epoch 50: Loss = 6.0794\n",
      "Year 2000, Epoch 60: Loss = 5.2666\n",
      "Year 2000, Epoch 70: Loss = 4.2050\n",
      "Year 2000, Epoch 80: Loss = 3.1983\n",
      "Year 2000, Epoch 90: Loss = 3.0505\n",
      "Embeddings saved for 2000!\n",
      "Training GCN for year 2001...\n",
      "Year 2001, Epoch 0: Loss = 681.8171\n",
      "Year 2001, Epoch 10: Loss = 159.1739\n",
      "Year 2001, Epoch 20: Loss = 14.6715\n",
      "Year 2001, Epoch 30: Loss = 15.4192\n",
      "Year 2001, Epoch 40: Loss = 11.9110\n",
      "Year 2001, Epoch 50: Loss = 8.0293\n",
      "Year 2001, Epoch 60: Loss = 7.3134\n",
      "Year 2001, Epoch 70: Loss = 6.7143\n",
      "Year 2001, Epoch 80: Loss = 4.7251\n",
      "Year 2001, Epoch 90: Loss = 3.9029\n",
      "Embeddings saved for 2001!\n",
      "Training GCN for year 2002...\n",
      "Year 2002, Epoch 0: Loss = 1162.6942\n",
      "Year 2002, Epoch 10: Loss = 277.4922\n",
      "Year 2002, Epoch 20: Loss = 17.4754\n",
      "Year 2002, Epoch 30: Loss = 29.5556\n",
      "Year 2002, Epoch 40: Loss = 16.9200\n",
      "Year 2002, Epoch 50: Loss = 5.8646\n",
      "Year 2002, Epoch 60: Loss = 5.5491\n",
      "Year 2002, Epoch 70: Loss = 3.7053\n",
      "Year 2002, Epoch 80: Loss = 3.7191\n",
      "Year 2002, Epoch 90: Loss = 3.4710\n",
      "Embeddings saved for 2002!\n",
      "Training GCN for year 2003...\n",
      "Year 2003, Epoch 0: Loss = 574.9026\n",
      "Year 2003, Epoch 10: Loss = 204.9183\n",
      "Year 2003, Epoch 20: Loss = 31.5343\n",
      "Year 2003, Epoch 30: Loss = 9.1177\n",
      "Year 2003, Epoch 40: Loss = 12.8433\n",
      "Year 2003, Epoch 50: Loss = 4.4655\n",
      "Year 2003, Epoch 60: Loss = 4.2369\n",
      "Year 2003, Epoch 70: Loss = 3.6927\n",
      "Year 2003, Epoch 80: Loss = 3.4468\n",
      "Year 2003, Epoch 90: Loss = 3.3239\n",
      "Embeddings saved for 2003!\n",
      "Training GCN for year 2004...\n",
      "Year 2004, Epoch 0: Loss = 217.6013\n",
      "Year 2004, Epoch 10: Loss = 20.4140\n",
      "Year 2004, Epoch 20: Loss = 9.2341\n",
      "Year 2004, Epoch 30: Loss = 7.2855\n",
      "Year 2004, Epoch 40: Loss = 3.4625\n",
      "Year 2004, Epoch 50: Loss = 3.7395\n",
      "Year 2004, Epoch 60: Loss = 3.0093\n",
      "Year 2004, Epoch 70: Loss = 2.9655\n",
      "Year 2004, Epoch 80: Loss = 2.8984\n",
      "Year 2004, Epoch 90: Loss = 2.8938\n",
      "Embeddings saved for 2004!\n",
      "Training GCN for year 2005...\n",
      "Year 2005, Epoch 0: Loss = 351.2061\n",
      "Year 2005, Epoch 10: Loss = 171.0448\n",
      "Year 2005, Epoch 20: Loss = 32.6344\n",
      "Year 2005, Epoch 30: Loss = 9.2097\n",
      "Year 2005, Epoch 40: Loss = 9.3064\n",
      "Year 2005, Epoch 50: Loss = 3.4132\n",
      "Year 2005, Epoch 60: Loss = 3.9300\n",
      "Year 2005, Epoch 70: Loss = 3.1056\n",
      "Year 2005, Epoch 80: Loss = 3.0245\n",
      "Year 2005, Epoch 90: Loss = 2.9517\n",
      "Embeddings saved for 2005!\n",
      "Training GCN for year 2006...\n",
      "Year 2006, Epoch 0: Loss = 955.1323\n",
      "Year 2006, Epoch 10: Loss = 303.0535\n",
      "Year 2006, Epoch 20: Loss = 40.8120\n",
      "Year 2006, Epoch 30: Loss = 7.7640\n",
      "Year 2006, Epoch 40: Loss = 15.4069\n",
      "Year 2006, Epoch 50: Loss = 5.8430\n",
      "Year 2006, Epoch 60: Loss = 5.0327\n",
      "Year 2006, Epoch 70: Loss = 4.9150\n",
      "Year 2006, Epoch 80: Loss = 4.2776\n",
      "Year 2006, Epoch 90: Loss = 4.1839\n",
      "Embeddings saved for 2006!\n",
      "Training GCN for year 2007...\n",
      "Year 2007, Epoch 0: Loss = 910.3295\n",
      "Year 2007, Epoch 10: Loss = 431.9874\n",
      "Year 2007, Epoch 20: Loss = 202.0629\n",
      "Year 2007, Epoch 30: Loss = 42.9404\n",
      "Year 2007, Epoch 40: Loss = 12.6294\n",
      "Year 2007, Epoch 50: Loss = 12.7962\n",
      "Year 2007, Epoch 60: Loss = 5.8296\n",
      "Year 2007, Epoch 70: Loss = 6.0975\n",
      "Year 2007, Epoch 80: Loss = 4.7654\n",
      "Year 2007, Epoch 90: Loss = 4.5752\n",
      "Embeddings saved for 2007!\n",
      "Training GCN for year 2008...\n",
      "Year 2008, Epoch 0: Loss = 387.4741\n",
      "Year 2008, Epoch 10: Loss = 210.5653\n",
      "Year 2008, Epoch 20: Loss = 92.7615\n",
      "Year 2008, Epoch 30: Loss = 16.2038\n",
      "Year 2008, Epoch 40: Loss = 8.1524\n",
      "Year 2008, Epoch 50: Loss = 7.9412\n",
      "Year 2008, Epoch 60: Loss = 4.5509\n",
      "Year 2008, Epoch 70: Loss = 4.8227\n",
      "Year 2008, Epoch 80: Loss = 4.2843\n",
      "Year 2008, Epoch 90: Loss = 4.1760\n",
      "Embeddings saved for 2008!\n",
      "Training GCN for year 2009...\n",
      "Year 2009, Epoch 0: Loss = 737.5041\n",
      "Year 2009, Epoch 10: Loss = 156.4062\n",
      "Year 2009, Epoch 20: Loss = 9.7696\n",
      "Year 2009, Epoch 30: Loss = 26.8277\n",
      "Year 2009, Epoch 40: Loss = 7.1228\n",
      "Year 2009, Epoch 50: Loss = 8.5612\n",
      "Year 2009, Epoch 60: Loss = 6.2845\n",
      "Year 2009, Epoch 70: Loss = 5.8769\n",
      "Year 2009, Epoch 80: Loss = 5.3936\n",
      "Year 2009, Epoch 90: Loss = 5.0766\n",
      "Embeddings saved for 2009!\n",
      "Training GCN for year 2010...\n",
      "Year 2010, Epoch 0: Loss = 106.8394\n",
      "Year 2010, Epoch 10: Loss = 11.9608\n",
      "Year 2010, Epoch 20: Loss = 9.4565\n",
      "Year 2010, Epoch 30: Loss = 6.7639\n",
      "Year 2010, Epoch 40: Loss = 4.2346\n",
      "Year 2010, Epoch 50: Loss = 3.5351\n",
      "Year 2010, Epoch 60: Loss = 3.1757\n",
      "Year 2010, Epoch 70: Loss = 3.1821\n",
      "Year 2010, Epoch 80: Loss = 3.1564\n",
      "Year 2010, Epoch 90: Loss = 3.1282\n",
      "Embeddings saved for 2010!\n",
      "Training GCN for year 2011...\n",
      "Year 2011, Epoch 0: Loss = 924.0690\n",
      "Year 2011, Epoch 10: Loss = 429.3000\n",
      "Year 2011, Epoch 20: Loss = 138.3893\n",
      "Year 2011, Epoch 30: Loss = 11.0725\n",
      "Year 2011, Epoch 40: Loss = 13.2730\n",
      "Year 2011, Epoch 50: Loss = 9.4228\n",
      "Year 2011, Epoch 60: Loss = 4.2063\n",
      "Year 2011, Epoch 70: Loss = 4.8726\n",
      "Year 2011, Epoch 80: Loss = 4.0506\n",
      "Year 2011, Epoch 90: Loss = 3.9532\n",
      "Embeddings saved for 2011!\n",
      "Training GCN for year 2012...\n",
      "Year 2012, Epoch 0: Loss = 697.4278\n",
      "Year 2012, Epoch 10: Loss = 343.0811\n",
      "Year 2012, Epoch 20: Loss = 159.9647\n",
      "Year 2012, Epoch 30: Loss = 50.7847\n",
      "Year 2012, Epoch 40: Loss = 8.6480\n",
      "Year 2012, Epoch 50: Loss = 6.2929\n",
      "Year 2012, Epoch 60: Loss = 7.2405\n",
      "Year 2012, Epoch 70: Loss = 5.2007\n",
      "Year 2012, Epoch 80: Loss = 4.7648\n",
      "Year 2012, Epoch 90: Loss = 4.7571\n",
      "Embeddings saved for 2012!\n",
      "Training GCN for year 2013...\n",
      "Year 2013, Epoch 0: Loss = 343.3044\n",
      "Year 2013, Epoch 10: Loss = 63.5990\n",
      "Year 2013, Epoch 20: Loss = 4.6370\n",
      "Year 2013, Epoch 30: Loss = 13.1689\n",
      "Year 2013, Epoch 40: Loss = 3.5585\n",
      "Year 2013, Epoch 50: Loss = 4.5310\n",
      "Year 2013, Epoch 60: Loss = 3.2645\n",
      "Year 2013, Epoch 70: Loss = 3.2732\n",
      "Year 2013, Epoch 80: Loss = 3.0424\n",
      "Year 2013, Epoch 90: Loss = 2.9938\n",
      "Embeddings saved for 2013!\n",
      "Training GCN for year 2014...\n",
      "Year 2014, Epoch 0: Loss = 1059.8098\n",
      "Year 2014, Epoch 10: Loss = 397.1071\n",
      "Year 2014, Epoch 20: Loss = 46.8933\n",
      "Year 2014, Epoch 30: Loss = 22.4830\n",
      "Year 2014, Epoch 40: Loss = 17.9996\n",
      "Year 2014, Epoch 50: Loss = 6.4843\n",
      "Year 2014, Epoch 60: Loss = 7.4179\n",
      "Year 2014, Epoch 70: Loss = 5.2508\n",
      "Year 2014, Epoch 80: Loss = 5.0749\n",
      "Year 2014, Epoch 90: Loss = 4.5236\n",
      "Embeddings saved for 2014!\n",
      "Training GCN for year 2015...\n",
      "Year 2015, Epoch 0: Loss = 848.8961\n",
      "Year 2015, Epoch 10: Loss = 284.6006\n",
      "Year 2015, Epoch 20: Loss = 27.2579\n",
      "Year 2015, Epoch 30: Loss = 18.9085\n",
      "Year 2015, Epoch 40: Loss = 5.7177\n",
      "Year 2015, Epoch 50: Loss = 6.5245\n",
      "Year 2015, Epoch 60: Loss = 4.0961\n",
      "Year 2015, Epoch 70: Loss = 3.7965\n",
      "Year 2015, Epoch 80: Loss = 3.5045\n",
      "Year 2015, Epoch 90: Loss = 3.2107\n",
      "Embeddings saved for 2015!\n",
      "Training GCN for year 2016...\n",
      "Year 2016, Epoch 0: Loss = 657.0605\n",
      "Year 2016, Epoch 10: Loss = 309.1566\n",
      "Year 2016, Epoch 20: Loss = 111.1254\n",
      "Year 2016, Epoch 30: Loss = 15.6324\n",
      "Year 2016, Epoch 40: Loss = 11.0394\n",
      "Year 2016, Epoch 50: Loss = 10.6903\n",
      "Year 2016, Epoch 60: Loss = 6.2046\n",
      "Year 2016, Epoch 70: Loss = 6.4347\n",
      "Year 2016, Epoch 80: Loss = 5.8375\n",
      "Year 2016, Epoch 90: Loss = 5.5182\n",
      "Embeddings saved for 2016!\n",
      "Training GCN for year 2017...\n",
      "Year 2017, Epoch 0: Loss = 497.1472\n",
      "Year 2017, Epoch 10: Loss = 211.7331\n",
      "Year 2017, Epoch 20: Loss = 43.4024\n",
      "Year 2017, Epoch 30: Loss = 14.4814\n",
      "Year 2017, Epoch 40: Loss = 11.4612\n",
      "Year 2017, Epoch 50: Loss = 6.0995\n",
      "Year 2017, Epoch 60: Loss = 5.6778\n",
      "Year 2017, Epoch 70: Loss = 4.4973\n",
      "Year 2017, Epoch 80: Loss = 4.1033\n",
      "Year 2017, Epoch 90: Loss = 3.6810\n",
      "Embeddings saved for 2017!\n",
      "Training GCN for year 2018...\n",
      "Year 2018, Epoch 0: Loss = 399.8203\n",
      "Year 2018, Epoch 10: Loss = 137.7531\n",
      "Year 2018, Epoch 20: Loss = 21.2932\n",
      "Year 2018, Epoch 30: Loss = 6.6301\n",
      "Year 2018, Epoch 40: Loss = 6.9070\n",
      "Year 2018, Epoch 50: Loss = 3.7041\n",
      "Year 2018, Epoch 60: Loss = 3.6405\n",
      "Year 2018, Epoch 70: Loss = 3.0480\n",
      "Year 2018, Epoch 80: Loss = 3.0166\n",
      "Year 2018, Epoch 90: Loss = 2.9126\n",
      "Embeddings saved for 2018!\n",
      "Training GCN for year 2019...\n",
      "Year 2019, Epoch 0: Loss = 118.8230\n",
      "Year 2019, Epoch 10: Loss = 17.8656\n",
      "Year 2019, Epoch 20: Loss = 8.3415\n",
      "Year 2019, Epoch 30: Loss = 8.7826\n",
      "Year 2019, Epoch 40: Loss = 5.9716\n",
      "Year 2019, Epoch 50: Loss = 4.6111\n",
      "Year 2019, Epoch 60: Loss = 3.9718\n",
      "Year 2019, Epoch 70: Loss = 3.4728\n",
      "Year 2019, Epoch 80: Loss = 3.1096\n",
      "Year 2019, Epoch 90: Loss = 2.8917\n",
      "Embeddings saved for 2019!\n",
      "Training GCN for year 2020...\n",
      "Year 2020, Epoch 0: Loss = 426.3253\n",
      "Year 2020, Epoch 10: Loss = 100.8576\n",
      "Year 2020, Epoch 20: Loss = 8.1898\n",
      "Year 2020, Epoch 30: Loss = 14.1731\n",
      "Year 2020, Epoch 40: Loss = 7.8661\n",
      "Year 2020, Epoch 50: Loss = 4.7495\n",
      "Year 2020, Epoch 60: Loss = 4.4016\n",
      "Year 2020, Epoch 70: Loss = 3.4892\n",
      "Year 2020, Epoch 80: Loss = 3.3038\n",
      "Year 2020, Epoch 90: Loss = 3.1176\n",
      "Embeddings saved for 2020!\n",
      "Training GCN for year 2021...\n",
      "Year 2021, Epoch 0: Loss = 571.1858\n",
      "Year 2021, Epoch 10: Loss = 207.8306\n",
      "Year 2021, Epoch 20: Loss = 45.7972\n",
      "Year 2021, Epoch 30: Loss = 7.8385\n",
      "Year 2021, Epoch 40: Loss = 11.1606\n",
      "Year 2021, Epoch 50: Loss = 6.2859\n",
      "Year 2021, Epoch 60: Loss = 4.7707\n",
      "Year 2021, Epoch 70: Loss = 4.8173\n",
      "Year 2021, Epoch 80: Loss = 4.3828\n",
      "Year 2021, Epoch 90: Loss = 4.2992\n",
      "Embeddings saved for 2021!\n",
      "Training GCN for year 2022...\n",
      "Year 2022, Epoch 0: Loss = 597.2945\n",
      "Year 2022, Epoch 10: Loss = 204.2294\n",
      "Year 2022, Epoch 20: Loss = 25.4772\n",
      "Year 2022, Epoch 30: Loss = 11.3720\n",
      "Year 2022, Epoch 40: Loss = 12.9114\n",
      "Year 2022, Epoch 50: Loss = 3.7577\n",
      "Year 2022, Epoch 60: Loss = 3.9956\n",
      "Year 2022, Epoch 70: Loss = 3.0518\n",
      "Year 2022, Epoch 80: Loss = 3.0877\n",
      "Year 2022, Epoch 90: Loss = 2.9829\n",
      "Embeddings saved for 2022!\n",
      "Training GCN for year 2023...\n",
      "Year 2023, Epoch 0: Loss = 471.0748\n",
      "Year 2023, Epoch 10: Loss = 65.9248\n",
      "Year 2023, Epoch 20: Loss = 21.6028\n",
      "Year 2023, Epoch 30: Loss = 12.4531\n",
      "Year 2023, Epoch 40: Loss = 7.1675\n",
      "Year 2023, Epoch 50: Loss = 4.7193\n",
      "Year 2023, Epoch 60: Loss = 4.0893\n",
      "Year 2023, Epoch 70: Loss = 3.1041\n",
      "Year 2023, Epoch 80: Loss = 2.8532\n",
      "Year 2023, Epoch 90: Loss = 2.7711\n",
      "Embeddings saved for 2023!\n",
      "GCN Training Completed!\n"
     ]
    }
   ],
   "source": [
    "for year in range(2000, 2024):\n",
    "    print(f\"Training GCN for year {year}...\")\n",
    "    graph = torch.load(f\"{graph_dir}/graph_{year}.pt\")\n",
    "    \n",
    "    model = GCN(in_channels=graph.x.shape[1], hidden_channels=hidden_dim, out_channels=output_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(graph.x, graph.edge_index)\n",
    "        loss = F.mse_loss(output, graph.x)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Year {year}, Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    \n",
    "    torch.save(output, f\"{embeddings_dir}/embeddings_{year}.pt\")\n",
    "    print(f\"Embeddings saved for {year}!\")\n",
    "\n",
    "print(\"GCN Training Completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
