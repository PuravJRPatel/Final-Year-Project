{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 20\n",
    "output_dim = 2  \n",
    "num_epochs = 400\n",
    "learning_rate = 0.01\n",
    "\n",
    "graph_dir = \"processed_graphs\"\n",
    "embeddings_dir = \"graph_embeddings\"\n",
    "os.makedirs(embeddings_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN for year 2000...\n",
      "Year 2000, Epoch 0: Loss = 748.1550\n",
      "Year 2000, Epoch 20: Loss = 53.1440\n",
      "Year 2000, Epoch 40: Loss = 12.7925\n",
      "Year 2000, Epoch 60: Loss = 4.1764\n",
      "Year 2000, Epoch 80: Loss = 3.1328\n",
      "Year 2000, Epoch 100: Loss = 3.0263\n",
      "Year 2000, Epoch 120: Loss = 3.0036\n",
      "Year 2000, Epoch 140: Loss = 2.9919\n",
      "Year 2000, Epoch 160: Loss = 2.9835\n",
      "Year 2000, Epoch 180: Loss = 2.9775\n",
      "Year 2000, Epoch 200: Loss = 2.9744\n",
      "Year 2000, Epoch 220: Loss = 2.9726\n",
      "Year 2000, Epoch 240: Loss = 2.9714\n",
      "Year 2000, Epoch 260: Loss = 2.9705\n",
      "Year 2000, Epoch 280: Loss = 2.9698\n",
      "Year 2000, Epoch 300: Loss = 2.9693\n",
      "Year 2000, Epoch 320: Loss = 2.9689\n",
      "Year 2000, Epoch 340: Loss = 2.9684\n",
      "Year 2000, Epoch 360: Loss = 2.9681\n",
      "Year 2000, Epoch 380: Loss = 2.9677\n",
      "Embeddings saved for 2000!\n",
      "Training GCN for year 2001...\n",
      "Year 2001, Epoch 0: Loss = 300.9543\n",
      "Year 2001, Epoch 20: Loss = 6.1071\n",
      "Year 2001, Epoch 40: Loss = 5.4119\n",
      "Year 2001, Epoch 60: Loss = 3.5629\n",
      "Year 2001, Epoch 80: Loss = 3.4211\n",
      "Year 2001, Epoch 100: Loss = 3.3212\n",
      "Year 2001, Epoch 120: Loss = 3.3103\n",
      "Year 2001, Epoch 140: Loss = 3.3094\n",
      "Year 2001, Epoch 160: Loss = 3.3086\n",
      "Year 2001, Epoch 180: Loss = 3.3075\n",
      "Year 2001, Epoch 200: Loss = 3.3071\n",
      "Year 2001, Epoch 220: Loss = 3.3067\n",
      "Year 2001, Epoch 240: Loss = 3.3064\n",
      "Year 2001, Epoch 260: Loss = 3.3060\n",
      "Year 2001, Epoch 280: Loss = 3.3056\n",
      "Year 2001, Epoch 300: Loss = 3.3052\n",
      "Year 2001, Epoch 320: Loss = 3.3048\n",
      "Year 2001, Epoch 340: Loss = 3.3044\n",
      "Year 2001, Epoch 360: Loss = 3.3040\n",
      "Year 2001, Epoch 380: Loss = 3.3036\n",
      "Embeddings saved for 2001!\n",
      "Training GCN for year 2002...\n",
      "Year 2002, Epoch 0: Loss = 861.4736\n",
      "Year 2002, Epoch 20: Loss = 340.6026\n",
      "Year 2002, Epoch 40: Loss = 90.9044\n",
      "Year 2002, Epoch 60: Loss = 11.4947\n",
      "Year 2002, Epoch 80: Loss = 7.2664\n",
      "Year 2002, Epoch 100: Loss = 5.9969\n",
      "Year 2002, Epoch 120: Loss = 4.0136\n",
      "Year 2002, Epoch 140: Loss = 3.4575\n",
      "Year 2002, Epoch 160: Loss = 3.4075\n",
      "Year 2002, Epoch 180: Loss = 3.3970\n",
      "Year 2002, Epoch 200: Loss = 3.3962\n",
      "Year 2002, Epoch 220: Loss = 3.3954\n",
      "Year 2002, Epoch 240: Loss = 3.3948\n",
      "Year 2002, Epoch 260: Loss = 3.3942\n",
      "Year 2002, Epoch 280: Loss = 3.3936\n",
      "Year 2002, Epoch 300: Loss = 3.3930\n",
      "Year 2002, Epoch 320: Loss = 3.3924\n",
      "Year 2002, Epoch 340: Loss = 3.3918\n",
      "Year 2002, Epoch 360: Loss = 3.3911\n",
      "Year 2002, Epoch 380: Loss = 3.3905\n",
      "Embeddings saved for 2002!\n",
      "Training GCN for year 2003...\n",
      "Year 2003, Epoch 0: Loss = 771.8689\n",
      "Year 2003, Epoch 20: Loss = 18.6158\n",
      "Year 2003, Epoch 40: Loss = 7.6661\n",
      "Year 2003, Epoch 60: Loss = 3.5333\n",
      "Year 2003, Epoch 80: Loss = 3.3410\n",
      "Year 2003, Epoch 100: Loss = 3.2483\n",
      "Year 2003, Epoch 120: Loss = 3.2309\n",
      "Year 2003, Epoch 140: Loss = 3.2231\n",
      "Year 2003, Epoch 160: Loss = 3.2192\n",
      "Year 2003, Epoch 180: Loss = 3.2153\n",
      "Year 2003, Epoch 200: Loss = 3.2114\n",
      "Year 2003, Epoch 220: Loss = 3.2077\n",
      "Year 2003, Epoch 240: Loss = 3.2043\n",
      "Year 2003, Epoch 260: Loss = 3.2016\n",
      "Year 2003, Epoch 280: Loss = 3.1994\n",
      "Year 2003, Epoch 300: Loss = 3.1978\n",
      "Year 2003, Epoch 320: Loss = 3.1966\n",
      "Year 2003, Epoch 340: Loss = 3.1956\n",
      "Year 2003, Epoch 360: Loss = 3.1948\n",
      "Year 2003, Epoch 380: Loss = 3.1940\n",
      "Embeddings saved for 2003!\n",
      "Training GCN for year 2004...\n",
      "Year 2004, Epoch 0: Loss = 278.1123\n",
      "Year 2004, Epoch 20: Loss = 19.9410\n",
      "Year 2004, Epoch 40: Loss = 6.0798\n",
      "Year 2004, Epoch 60: Loss = 3.2519\n",
      "Year 2004, Epoch 80: Loss = 2.9450\n",
      "Year 2004, Epoch 100: Loss = 2.9034\n",
      "Year 2004, Epoch 120: Loss = 2.8911\n",
      "Year 2004, Epoch 140: Loss = 2.8833\n",
      "Year 2004, Epoch 160: Loss = 2.8764\n",
      "Year 2004, Epoch 180: Loss = 2.8717\n",
      "Year 2004, Epoch 200: Loss = 2.8684\n",
      "Year 2004, Epoch 220: Loss = 2.8656\n",
      "Year 2004, Epoch 240: Loss = 2.8634\n",
      "Year 2004, Epoch 260: Loss = 2.8619\n",
      "Year 2004, Epoch 280: Loss = 2.8608\n",
      "Year 2004, Epoch 300: Loss = 2.8599\n",
      "Year 2004, Epoch 320: Loss = 2.8592\n",
      "Year 2004, Epoch 340: Loss = 2.8586\n",
      "Year 2004, Epoch 360: Loss = 2.8581\n",
      "Year 2004, Epoch 380: Loss = 2.8577\n",
      "Embeddings saved for 2004!\n",
      "Training GCN for year 2005...\n",
      "Year 2005, Epoch 0: Loss = 425.2694\n",
      "Year 2005, Epoch 20: Loss = 11.1997\n",
      "Year 2005, Epoch 40: Loss = 5.3043\n",
      "Year 2005, Epoch 60: Loss = 3.6098\n",
      "Year 2005, Epoch 80: Loss = 3.0046\n",
      "Year 2005, Epoch 100: Loss = 2.9436\n",
      "Year 2005, Epoch 120: Loss = 2.9260\n",
      "Year 2005, Epoch 140: Loss = 2.9152\n",
      "Year 2005, Epoch 160: Loss = 2.9089\n",
      "Year 2005, Epoch 180: Loss = 2.9048\n",
      "Year 2005, Epoch 200: Loss = 2.9012\n",
      "Year 2005, Epoch 220: Loss = 2.8987\n",
      "Year 2005, Epoch 240: Loss = 2.8973\n",
      "Year 2005, Epoch 260: Loss = 2.8963\n",
      "Year 2005, Epoch 280: Loss = 2.8956\n",
      "Year 2005, Epoch 300: Loss = 2.8950\n",
      "Year 2005, Epoch 320: Loss = 2.8945\n",
      "Year 2005, Epoch 340: Loss = 2.8939\n",
      "Year 2005, Epoch 360: Loss = 2.8934\n",
      "Year 2005, Epoch 380: Loss = 2.8930\n",
      "Embeddings saved for 2005!\n",
      "Training GCN for year 2006...\n",
      "Year 2006, Epoch 0: Loss = 499.7863\n",
      "Year 2006, Epoch 20: Loss = 12.4263\n",
      "Year 2006, Epoch 40: Loss = 8.3100\n",
      "Year 2006, Epoch 60: Loss = 5.8671\n",
      "Year 2006, Epoch 80: Loss = 4.6508\n",
      "Year 2006, Epoch 100: Loss = 3.9923\n",
      "Year 2006, Epoch 120: Loss = 3.5677\n",
      "Year 2006, Epoch 140: Loss = 3.3132\n",
      "Year 2006, Epoch 160: Loss = 3.1784\n",
      "Year 2006, Epoch 180: Loss = 3.1150\n",
      "Year 2006, Epoch 200: Loss = 3.0864\n",
      "Year 2006, Epoch 220: Loss = 3.0725\n",
      "Year 2006, Epoch 240: Loss = 3.0649\n",
      "Year 2006, Epoch 260: Loss = 3.0603\n",
      "Year 2006, Epoch 280: Loss = 3.0574\n",
      "Year 2006, Epoch 300: Loss = 3.0554\n",
      "Year 2006, Epoch 320: Loss = 3.0540\n",
      "Year 2006, Epoch 340: Loss = 3.0529\n",
      "Year 2006, Epoch 360: Loss = 3.0520\n",
      "Year 2006, Epoch 380: Loss = 3.0512\n",
      "Embeddings saved for 2006!\n",
      "Training GCN for year 2007...\n",
      "Year 2007, Epoch 0: Loss = 631.9212\n",
      "Year 2007, Epoch 20: Loss = 14.3397\n",
      "Year 2007, Epoch 40: Loss = 8.8406\n",
      "Year 2007, Epoch 60: Loss = 7.2774\n",
      "Year 2007, Epoch 80: Loss = 6.1078\n",
      "Year 2007, Epoch 100: Loss = 5.3217\n",
      "Year 2007, Epoch 120: Loss = 4.6270\n",
      "Year 2007, Epoch 140: Loss = 4.0087\n",
      "Year 2007, Epoch 160: Loss = 3.5145\n",
      "Year 2007, Epoch 180: Loss = 3.1699\n",
      "Year 2007, Epoch 200: Loss = 2.9786\n",
      "Year 2007, Epoch 220: Loss = 2.9004\n",
      "Year 2007, Epoch 240: Loss = 2.8759\n",
      "Year 2007, Epoch 260: Loss = 2.8684\n",
      "Year 2007, Epoch 280: Loss = 2.8654\n",
      "Year 2007, Epoch 300: Loss = 2.8638\n",
      "Year 2007, Epoch 320: Loss = 2.8630\n",
      "Year 2007, Epoch 340: Loss = 2.8623\n",
      "Year 2007, Epoch 360: Loss = 2.8618\n",
      "Year 2007, Epoch 380: Loss = 2.8614\n",
      "Embeddings saved for 2007!\n",
      "Training GCN for year 2008...\n",
      "Year 2008, Epoch 0: Loss = 703.0743\n",
      "Year 2008, Epoch 20: Loss = 7.5534\n",
      "Year 2008, Epoch 40: Loss = 7.3298\n",
      "Year 2008, Epoch 60: Loss = 5.2666\n",
      "Year 2008, Epoch 80: Loss = 4.2229\n",
      "Year 2008, Epoch 100: Loss = 3.7363\n",
      "Year 2008, Epoch 120: Loss = 3.4187\n",
      "Year 2008, Epoch 140: Loss = 3.1964\n",
      "Year 2008, Epoch 160: Loss = 3.0425\n",
      "Year 2008, Epoch 180: Loss = 2.9368\n",
      "Year 2008, Epoch 200: Loss = 2.8634\n",
      "Year 2008, Epoch 220: Loss = 2.8116\n",
      "Year 2008, Epoch 240: Loss = 2.7751\n",
      "Year 2008, Epoch 260: Loss = 2.7498\n",
      "Year 2008, Epoch 280: Loss = 2.7325\n",
      "Year 2008, Epoch 300: Loss = 2.7211\n",
      "Year 2008, Epoch 320: Loss = 2.7136\n",
      "Year 2008, Epoch 340: Loss = 2.7089\n",
      "Year 2008, Epoch 360: Loss = 2.7060\n",
      "Year 2008, Epoch 380: Loss = 2.7041\n",
      "Embeddings saved for 2008!\n",
      "Training GCN for year 2009...\n",
      "Year 2009, Epoch 0: Loss = 853.3121\n",
      "Year 2009, Epoch 20: Loss = 38.3180\n",
      "Year 2009, Epoch 40: Loss = 12.5770\n",
      "Year 2009, Epoch 60: Loss = 4.2500\n",
      "Year 2009, Epoch 80: Loss = 3.0292\n",
      "Year 2009, Epoch 100: Loss = 2.9649\n",
      "Year 2009, Epoch 120: Loss = 2.9406\n",
      "Year 2009, Epoch 140: Loss = 2.9389\n",
      "Year 2009, Epoch 160: Loss = 2.9385\n",
      "Year 2009, Epoch 180: Loss = 2.9381\n",
      "Year 2009, Epoch 200: Loss = 2.9378\n",
      "Year 2009, Epoch 220: Loss = 2.9376\n",
      "Year 2009, Epoch 240: Loss = 2.9373\n",
      "Year 2009, Epoch 260: Loss = 2.9371\n",
      "Year 2009, Epoch 280: Loss = 2.9368\n",
      "Year 2009, Epoch 300: Loss = 2.9366\n",
      "Year 2009, Epoch 320: Loss = 2.9364\n",
      "Year 2009, Epoch 340: Loss = 2.9361\n",
      "Year 2009, Epoch 360: Loss = 2.9359\n",
      "Year 2009, Epoch 380: Loss = 2.9357\n",
      "Embeddings saved for 2009!\n",
      "Training GCN for year 2010...\n",
      "Year 2010, Epoch 0: Loss = 1017.6976\n",
      "Year 2010, Epoch 20: Loss = 9.9246\n",
      "Year 2010, Epoch 40: Loss = 6.2710\n",
      "Year 2010, Epoch 60: Loss = 4.1268\n",
      "Year 2010, Epoch 80: Loss = 3.3759\n",
      "Year 2010, Epoch 100: Loss = 3.1818\n",
      "Year 2010, Epoch 120: Loss = 3.1369\n",
      "Year 2010, Epoch 140: Loss = 3.1273\n",
      "Year 2010, Epoch 160: Loss = 3.1247\n",
      "Year 2010, Epoch 180: Loss = 3.1236\n",
      "Year 2010, Epoch 200: Loss = 3.1228\n",
      "Year 2010, Epoch 220: Loss = 3.1221\n",
      "Year 2010, Epoch 240: Loss = 3.1215\n",
      "Year 2010, Epoch 260: Loss = 3.1208\n",
      "Year 2010, Epoch 280: Loss = 3.1202\n",
      "Year 2010, Epoch 300: Loss = 3.1195\n",
      "Year 2010, Epoch 320: Loss = 3.1187\n",
      "Year 2010, Epoch 340: Loss = 3.1179\n",
      "Year 2010, Epoch 360: Loss = 3.1170\n",
      "Year 2010, Epoch 380: Loss = 3.1157\n",
      "Embeddings saved for 2010!\n",
      "Training GCN for year 2011...\n",
      "Year 2011, Epoch 0: Loss = 162.1243\n",
      "Year 2011, Epoch 20: Loss = 7.7693\n",
      "Year 2011, Epoch 40: Loss = 3.2123\n",
      "Year 2011, Epoch 60: Loss = 2.9860\n",
      "Year 2011, Epoch 80: Loss = 2.7377\n",
      "Year 2011, Epoch 100: Loss = 2.6891\n",
      "Year 2011, Epoch 120: Loss = 2.6867\n",
      "Year 2011, Epoch 140: Loss = 2.6855\n",
      "Year 2011, Epoch 160: Loss = 2.6847\n",
      "Year 2011, Epoch 180: Loss = 2.6838\n",
      "Year 2011, Epoch 200: Loss = 2.6829\n",
      "Year 2011, Epoch 220: Loss = 2.6819\n",
      "Year 2011, Epoch 240: Loss = 2.6808\n",
      "Year 2011, Epoch 260: Loss = 2.6798\n",
      "Year 2011, Epoch 280: Loss = 2.6788\n",
      "Year 2011, Epoch 300: Loss = 2.6777\n",
      "Year 2011, Epoch 320: Loss = 2.6768\n",
      "Year 2011, Epoch 340: Loss = 2.6760\n",
      "Year 2011, Epoch 360: Loss = 2.6751\n",
      "Year 2011, Epoch 380: Loss = 2.6743\n",
      "Embeddings saved for 2011!\n",
      "Training GCN for year 2012...\n",
      "Year 2012, Epoch 0: Loss = 969.8365\n",
      "Year 2012, Epoch 20: Loss = 43.9659\n",
      "Year 2012, Epoch 40: Loss = 9.0226\n",
      "Year 2012, Epoch 60: Loss = 5.0856\n",
      "Year 2012, Epoch 80: Loss = 4.3103\n",
      "Year 2012, Epoch 100: Loss = 3.9804\n",
      "Year 2012, Epoch 120: Loss = 3.7457\n",
      "Year 2012, Epoch 140: Loss = 3.5362\n",
      "Year 2012, Epoch 160: Loss = 3.3473\n",
      "Year 2012, Epoch 180: Loss = 3.1816\n",
      "Year 2012, Epoch 200: Loss = 3.0400\n",
      "Year 2012, Epoch 220: Loss = 2.9226\n",
      "Year 2012, Epoch 240: Loss = 2.8284\n",
      "Year 2012, Epoch 260: Loss = 2.7555\n",
      "Year 2012, Epoch 280: Loss = 2.7014\n",
      "Year 2012, Epoch 300: Loss = 2.6628\n",
      "Year 2012, Epoch 320: Loss = 2.6363\n",
      "Year 2012, Epoch 340: Loss = 2.6189\n",
      "Year 2012, Epoch 360: Loss = 2.6079\n",
      "Year 2012, Epoch 380: Loss = 2.6011\n",
      "Embeddings saved for 2012!\n",
      "Training GCN for year 2013...\n",
      "Year 2013, Epoch 0: Loss = 389.6984\n",
      "Year 2013, Epoch 20: Loss = 6.2279\n",
      "Year 2013, Epoch 40: Loss = 4.6110\n",
      "Year 2013, Epoch 60: Loss = 4.0631\n",
      "Year 2013, Epoch 80: Loss = 3.6719\n",
      "Year 2013, Epoch 100: Loss = 3.4252\n",
      "Year 2013, Epoch 120: Loss = 3.2370\n",
      "Year 2013, Epoch 140: Loss = 3.0907\n",
      "Year 2013, Epoch 160: Loss = 2.9804\n",
      "Year 2013, Epoch 180: Loss = 2.8992\n",
      "Year 2013, Epoch 200: Loss = 2.8409\n",
      "Year 2013, Epoch 220: Loss = 2.8002\n",
      "Year 2013, Epoch 240: Loss = 2.7723\n",
      "Year 2013, Epoch 260: Loss = 2.7538\n",
      "Year 2013, Epoch 280: Loss = 2.7417\n",
      "Year 2013, Epoch 300: Loss = 2.7339\n",
      "Year 2013, Epoch 320: Loss = 2.7291\n",
      "Year 2013, Epoch 340: Loss = 2.7260\n",
      "Year 2013, Epoch 360: Loss = 2.7241\n",
      "Year 2013, Epoch 380: Loss = 2.7228\n",
      "Embeddings saved for 2013!\n",
      "Training GCN for year 2014...\n",
      "Year 2014, Epoch 0: Loss = 1024.2339\n",
      "Year 2014, Epoch 20: Loss = 196.5297\n",
      "Year 2014, Epoch 40: Loss = 8.0251\n",
      "Year 2014, Epoch 60: Loss = 7.6804\n",
      "Year 2014, Epoch 80: Loss = 5.6949\n",
      "Year 2014, Epoch 100: Loss = 4.9534\n",
      "Year 2014, Epoch 120: Loss = 4.4880\n",
      "Year 2014, Epoch 140: Loss = 4.0657\n",
      "Year 2014, Epoch 160: Loss = 3.5549\n",
      "Year 2014, Epoch 180: Loss = 2.9841\n",
      "Year 2014, Epoch 200: Loss = 2.7604\n",
      "Year 2014, Epoch 220: Loss = 2.7122\n",
      "Year 2014, Epoch 240: Loss = 2.7082\n",
      "Year 2014, Epoch 260: Loss = 2.7074\n",
      "Year 2014, Epoch 280: Loss = 2.7069\n",
      "Year 2014, Epoch 300: Loss = 2.7064\n",
      "Year 2014, Epoch 320: Loss = 2.7059\n",
      "Year 2014, Epoch 340: Loss = 2.7055\n",
      "Year 2014, Epoch 360: Loss = 2.7051\n",
      "Year 2014, Epoch 380: Loss = 2.7047\n",
      "Embeddings saved for 2014!\n",
      "Training GCN for year 2015...\n",
      "Year 2015, Epoch 0: Loss = 595.6651\n",
      "Year 2015, Epoch 20: Loss = 33.7148\n",
      "Year 2015, Epoch 40: Loss = 13.8098\n",
      "Year 2015, Epoch 60: Loss = 7.6033\n",
      "Year 2015, Epoch 80: Loss = 5.5024\n",
      "Year 2015, Epoch 100: Loss = 4.5136\n",
      "Year 2015, Epoch 120: Loss = 3.9048\n",
      "Year 2015, Epoch 140: Loss = 3.5075\n",
      "Year 2015, Epoch 160: Loss = 3.2464\n",
      "Year 2015, Epoch 180: Loss = 3.0744\n",
      "Year 2015, Epoch 200: Loss = 2.9639\n",
      "Year 2015, Epoch 220: Loss = 2.8925\n",
      "Year 2015, Epoch 240: Loss = 2.8473\n",
      "Year 2015, Epoch 260: Loss = 2.8193\n",
      "Year 2015, Epoch 280: Loss = 2.8019\n",
      "Year 2015, Epoch 300: Loss = 2.7910\n",
      "Year 2015, Epoch 320: Loss = 2.7841\n",
      "Year 2015, Epoch 340: Loss = 2.7796\n",
      "Year 2015, Epoch 360: Loss = 2.7765\n",
      "Year 2015, Epoch 380: Loss = 2.7741\n",
      "Embeddings saved for 2015!\n",
      "Training GCN for year 2016...\n",
      "Year 2016, Epoch 0: Loss = 380.4871\n",
      "Year 2016, Epoch 20: Loss = 21.3251\n",
      "Year 2016, Epoch 40: Loss = 5.5680\n",
      "Year 2016, Epoch 60: Loss = 3.2011\n",
      "Year 2016, Epoch 80: Loss = 3.0236\n",
      "Year 2016, Epoch 100: Loss = 2.9179\n",
      "Year 2016, Epoch 120: Loss = 2.8839\n",
      "Year 2016, Epoch 140: Loss = 2.8730\n",
      "Year 2016, Epoch 160: Loss = 2.8702\n",
      "Year 2016, Epoch 180: Loss = 2.8694\n",
      "Year 2016, Epoch 200: Loss = 2.8688\n",
      "Year 2016, Epoch 220: Loss = 2.8683\n",
      "Year 2016, Epoch 240: Loss = 2.8676\n",
      "Year 2016, Epoch 260: Loss = 2.8670\n",
      "Year 2016, Epoch 280: Loss = 2.8664\n",
      "Year 2016, Epoch 300: Loss = 2.8658\n",
      "Year 2016, Epoch 320: Loss = 2.8651\n",
      "Year 2016, Epoch 340: Loss = 2.8644\n",
      "Year 2016, Epoch 360: Loss = 2.8638\n",
      "Year 2016, Epoch 380: Loss = 2.8633\n",
      "Embeddings saved for 2016!\n",
      "Training GCN for year 2017...\n",
      "Year 2017, Epoch 0: Loss = 796.4072\n",
      "Year 2017, Epoch 20: Loss = 215.2609\n",
      "Year 2017, Epoch 40: Loss = 9.4789\n",
      "Year 2017, Epoch 60: Loss = 7.3435\n",
      "Year 2017, Epoch 80: Loss = 6.2492\n",
      "Year 2017, Epoch 100: Loss = 5.2129\n",
      "Year 2017, Epoch 120: Loss = 4.5638\n",
      "Year 2017, Epoch 140: Loss = 4.1303\n",
      "Year 2017, Epoch 160: Loss = 3.8094\n",
      "Year 2017, Epoch 180: Loss = 3.5598\n",
      "Year 2017, Epoch 200: Loss = 3.3682\n",
      "Year 2017, Epoch 220: Loss = 3.2247\n",
      "Year 2017, Epoch 240: Loss = 3.1195\n",
      "Year 2017, Epoch 260: Loss = 3.0439\n",
      "Year 2017, Epoch 280: Loss = 2.9907\n",
      "Year 2017, Epoch 300: Loss = 2.9537\n",
      "Year 2017, Epoch 320: Loss = 2.9281\n",
      "Year 2017, Epoch 340: Loss = 2.9104\n",
      "Year 2017, Epoch 360: Loss = 2.8981\n",
      "Year 2017, Epoch 380: Loss = 2.8894\n",
      "Embeddings saved for 2017!\n",
      "Training GCN for year 2018...\n",
      "Year 2018, Epoch 0: Loss = 193.4600\n",
      "Year 2018, Epoch 20: Loss = 18.4091\n",
      "Year 2018, Epoch 40: Loss = 5.4060\n",
      "Year 2018, Epoch 60: Loss = 3.4114\n",
      "Year 2018, Epoch 80: Loss = 2.9718\n",
      "Year 2018, Epoch 100: Loss = 2.8313\n",
      "Year 2018, Epoch 120: Loss = 2.7845\n",
      "Year 2018, Epoch 140: Loss = 2.7697\n",
      "Year 2018, Epoch 160: Loss = 2.7659\n",
      "Year 2018, Epoch 180: Loss = 2.7649\n",
      "Year 2018, Epoch 200: Loss = 2.7646\n",
      "Year 2018, Epoch 220: Loss = 2.7643\n",
      "Year 2018, Epoch 240: Loss = 2.7640\n",
      "Year 2018, Epoch 260: Loss = 2.7637\n",
      "Year 2018, Epoch 280: Loss = 2.7634\n",
      "Year 2018, Epoch 300: Loss = 2.7630\n",
      "Year 2018, Epoch 320: Loss = 2.7627\n",
      "Year 2018, Epoch 340: Loss = 2.7623\n",
      "Year 2018, Epoch 360: Loss = 2.7619\n",
      "Year 2018, Epoch 380: Loss = 2.7615\n",
      "Embeddings saved for 2018!\n",
      "Training GCN for year 2019...\n",
      "Year 2019, Epoch 0: Loss = 780.5659\n",
      "Year 2019, Epoch 20: Loss = 156.6171\n",
      "Year 2019, Epoch 40: Loss = 8.6391\n",
      "Year 2019, Epoch 60: Loss = 8.2913\n",
      "Year 2019, Epoch 80: Loss = 5.6982\n",
      "Year 2019, Epoch 100: Loss = 5.0758\n",
      "Year 2019, Epoch 120: Loss = 4.6066\n",
      "Year 2019, Epoch 140: Loss = 4.1766\n",
      "Year 2019, Epoch 160: Loss = 3.8172\n",
      "Year 2019, Epoch 180: Loss = 3.5399\n",
      "Year 2019, Epoch 200: Loss = 3.3337\n",
      "Year 2019, Epoch 220: Loss = 3.1469\n",
      "Year 2019, Epoch 240: Loss = 2.9954\n",
      "Year 2019, Epoch 260: Loss = 2.8724\n",
      "Year 2019, Epoch 280: Loss = 2.7936\n",
      "Year 2019, Epoch 300: Loss = 2.7771\n",
      "Year 2019, Epoch 320: Loss = 2.7695\n",
      "Year 2019, Epoch 340: Loss = 2.7630\n",
      "Year 2019, Epoch 360: Loss = 2.7572\n",
      "Year 2019, Epoch 380: Loss = 2.7521\n",
      "Embeddings saved for 2019!\n",
      "Training GCN for year 2020...\n",
      "Year 2020, Epoch 0: Loss = 389.6345\n",
      "Year 2020, Epoch 20: Loss = 15.9268\n",
      "Year 2020, Epoch 40: Loss = 5.9549\n",
      "Year 2020, Epoch 60: Loss = 4.3362\n",
      "Year 2020, Epoch 80: Loss = 3.8066\n",
      "Year 2020, Epoch 100: Loss = 3.4765\n",
      "Year 2020, Epoch 120: Loss = 3.2524\n",
      "Year 2020, Epoch 140: Loss = 3.1034\n",
      "Year 2020, Epoch 160: Loss = 3.0069\n",
      "Year 2020, Epoch 180: Loss = 2.9472\n",
      "Year 2020, Epoch 200: Loss = 2.9119\n",
      "Year 2020, Epoch 220: Loss = 2.8915\n",
      "Year 2020, Epoch 240: Loss = 2.8799\n",
      "Year 2020, Epoch 260: Loss = 2.8732\n",
      "Year 2020, Epoch 280: Loss = 2.8689\n",
      "Year 2020, Epoch 300: Loss = 2.8660\n",
      "Year 2020, Epoch 320: Loss = 2.8638\n",
      "Year 2020, Epoch 340: Loss = 2.8618\n",
      "Year 2020, Epoch 360: Loss = 2.8601\n",
      "Year 2020, Epoch 380: Loss = 2.8586\n",
      "Embeddings saved for 2020!\n",
      "Training GCN for year 2021...\n",
      "Year 2021, Epoch 0: Loss = 493.9973\n",
      "Year 2021, Epoch 20: Loss = 12.3689\n",
      "Year 2021, Epoch 40: Loss = 3.1873\n",
      "Year 2021, Epoch 60: Loss = 2.9054\n",
      "Year 2021, Epoch 80: Loss = 2.8152\n",
      "Year 2021, Epoch 100: Loss = 2.7854\n",
      "Year 2021, Epoch 120: Loss = 2.7734\n",
      "Year 2021, Epoch 140: Loss = 2.7695\n",
      "Year 2021, Epoch 160: Loss = 2.7669\n",
      "Year 2021, Epoch 180: Loss = 2.7649\n",
      "Year 2021, Epoch 200: Loss = 2.7634\n",
      "Year 2021, Epoch 220: Loss = 2.7621\n",
      "Year 2021, Epoch 240: Loss = 2.7610\n",
      "Year 2021, Epoch 260: Loss = 2.7601\n",
      "Year 2021, Epoch 280: Loss = 2.7593\n",
      "Year 2021, Epoch 300: Loss = 2.7585\n",
      "Year 2021, Epoch 320: Loss = 2.7579\n",
      "Year 2021, Epoch 340: Loss = 2.7572\n",
      "Year 2021, Epoch 360: Loss = 2.7566\n",
      "Year 2021, Epoch 380: Loss = 2.7560\n",
      "Embeddings saved for 2021!\n",
      "Training GCN for year 2022...\n",
      "Year 2022, Epoch 0: Loss = 882.6442\n",
      "Year 2022, Epoch 20: Loss = 113.8024\n",
      "Year 2022, Epoch 40: Loss = 22.4753\n",
      "Year 2022, Epoch 60: Loss = 8.0640\n",
      "Year 2022, Epoch 80: Loss = 5.0753\n",
      "Year 2022, Epoch 100: Loss = 3.9945\n",
      "Year 2022, Epoch 120: Loss = 3.3662\n",
      "Year 2022, Epoch 140: Loss = 3.0437\n",
      "Year 2022, Epoch 160: Loss = 2.9139\n",
      "Year 2022, Epoch 180: Loss = 2.8749\n",
      "Year 2022, Epoch 200: Loss = 2.8655\n",
      "Year 2022, Epoch 220: Loss = 2.8627\n",
      "Year 2022, Epoch 240: Loss = 2.8613\n",
      "Year 2022, Epoch 260: Loss = 2.8603\n",
      "Year 2022, Epoch 280: Loss = 2.8594\n",
      "Year 2022, Epoch 300: Loss = 2.8587\n",
      "Year 2022, Epoch 320: Loss = 2.8581\n",
      "Year 2022, Epoch 340: Loss = 2.8575\n",
      "Year 2022, Epoch 360: Loss = 2.8570\n",
      "Year 2022, Epoch 380: Loss = 2.8565\n",
      "Embeddings saved for 2022!\n",
      "Training GCN for year 2023...\n",
      "Year 2023, Epoch 0: Loss = 1004.1123\n",
      "Year 2023, Epoch 20: Loss = 11.7363\n",
      "Year 2023, Epoch 40: Loss = 4.1740\n",
      "Year 2023, Epoch 60: Loss = 3.8097\n",
      "Year 2023, Epoch 80: Loss = 3.4476\n",
      "Year 2023, Epoch 100: Loss = 3.2094\n",
      "Year 2023, Epoch 120: Loss = 3.0369\n",
      "Year 2023, Epoch 140: Loss = 2.9193\n",
      "Year 2023, Epoch 160: Loss = 2.8443\n",
      "Year 2023, Epoch 180: Loss = 2.7988\n",
      "Year 2023, Epoch 200: Loss = 2.7722\n",
      "Year 2023, Epoch 220: Loss = 2.7571\n",
      "Year 2023, Epoch 240: Loss = 2.7485\n",
      "Year 2023, Epoch 260: Loss = 2.7434\n",
      "Year 2023, Epoch 280: Loss = 2.7401\n",
      "Year 2023, Epoch 300: Loss = 2.7378\n",
      "Year 2023, Epoch 320: Loss = 2.7360\n",
      "Year 2023, Epoch 340: Loss = 2.7346\n",
      "Year 2023, Epoch 360: Loss = 2.7336\n",
      "Year 2023, Epoch 380: Loss = 2.7329\n",
      "Embeddings saved for 2023!\n",
      "GCN Training Completed!\n"
     ]
    }
   ],
   "source": [
    "for year in range(2000, 2024):\n",
    "    print(f\"Training GCN for year {year}...\")\n",
    "    graph = torch.load(f\"{graph_dir}/graph_{year}.pt\")\n",
    "    \n",
    "    model = GCN(in_channels=graph.x.shape[1], hidden_channels=hidden_dim, out_channels=output_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(graph.x, graph.edge_index)\n",
    "        loss = F.mse_loss(output, graph.x)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Year {year}, Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    \n",
    "    torch.save(output, f\"{embeddings_dir}/embeddings_{year}.pt\")\n",
    "    print(f\"Embeddings saved for {year}!\")\n",
    "\n",
    "print(\"GCN Training Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 484.9755\n",
      "Epoch 50: Loss = 245.8815\n",
      "Epoch 100: Loss = 107.8632\n",
      "Epoch 150: Loss = 42.9638\n",
      "Epoch 200: Loss = 15.3817\n",
      "Epoch 250: Loss = 5.1223\n",
      "Epoch 300: Loss = 1.8128\n",
      "Epoch 350: Loss = 0.8854\n",
      "Epoch 400: Loss = 0.6576\n",
      "Epoch 450: Loss = 0.6079\n",
      "LSTM training completed and model saved!\n",
      "Predictions for 2024 saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=16, num_layers=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        output = self.fc(lstm_out[:, -1, :])  \n",
    "        return output\n",
    "\n",
    "\n",
    "graph_dir = \"graph_embeddings\"\n",
    "years = list(range(2000, 2024))\n",
    "embeddings = {}\n",
    "for year in years:\n",
    "    embeddings[year] = torch.load(os.path.join(graph_dir, f\"embeddings_{year}.pt\"))\n",
    "\n",
    "\n",
    "window_size = 10  \n",
    "train_x, train_y = [], []\n",
    "for i in range(len(years) - window_size):\n",
    "    x_seq = [embeddings[years[i + j]] for j in range(window_size)]  # Past 10 years\n",
    "    y_seq = embeddings[years[i + window_size]]  # Next year\n",
    "    train_x.append(torch.stack(x_seq))\n",
    "    train_y.append(y_seq)\n",
    "\n",
    "train_x = torch.stack(train_x)  \n",
    "train_y = torch.stack(train_y)  \n",
    "\n",
    "\n",
    "num_samples, window_size, num_nodes, features = train_x.shape\n",
    "train_x = train_x.reshape(num_samples, window_size, num_nodes * features)  \n",
    "input_dim = train_x.shape[-1]  \n",
    "lstm_model = LSTMModel(input_dim)\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    lstm_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = lstm_model(train_x)  # Shape: (num_samples, num_nodes * features)\n",
    "    \n",
    "    # Reshape predictions to match train_y\n",
    "    predictions = predictions.reshape(num_samples, num_nodes, features)  # Shape: (num_samples, num_nodes, features)\n",
    "    \n",
    "    loss = criterion(predictions, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# Save trained LSTM model\n",
    "torch.save(lstm_model.state_dict(), \"lstm_model.pth\")\n",
    "print(\"LSTM training completed and model saved!\")\n",
    "\n",
    "# Predict next year's embeddings (2024)\n",
    "past_10_years = [embeddings[year] for year in range(2014, 2024)]\n",
    "input_seq = torch.stack(past_10_years).unsqueeze(0)  # Shape: (1, 10, num_nodes, features)\n",
    "input_seq = input_seq.reshape(1, window_size, num_nodes * features)  # Reshape for LSTM\n",
    "\n",
    "lstm_model.eval()\n",
    "predicted_2024 = lstm_model(input_seq).squeeze(0)  # Shape: (num_nodes * features,)\n",
    "predicted_2024 = predicted_2024.reshape(num_nodes, features)  # Reshape to (num_nodes, features)\n",
    "\n",
    "# Save predicted embeddings for 2024\n",
    "torch.save(predicted_2024, \"graph_embeddings/embeddings_2024.pt\")\n",
    "print(\"Predictions for 2024 saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
